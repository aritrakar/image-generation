{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITZ424RmCapp"
      },
      "source": [
        "# Exploring Transformers for Sequence Modeling\n",
        "\n",
        "In this project, I delve into the implementation and training of Transformer models. Transformers, introduced by Vaswani et al. in 2017, have revolutionized the field of natural language processing (NLP) by enabling efficient parallel processing of sequences.\n",
        "\n",
        "This project aims to implement a Transformer model and train it on a sequence modeling task. The Transformer model's architecture, based on self-attention mechanisms, is particularly well-suited for tasks such as language modeling, translation, and more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsZFDop0Capq"
      },
      "source": [
        "### Multi-Head Attention\n",
        "\n",
        "The multi-head attention mechanism is a core component of the Transformer model. It allows the model to jointly attend to information from different representation subspaces at different positions. Here's a breakdown of how it works:\n",
        "\n",
        "1. **Scaled Dot-Product Attention:**\n",
        "   - The basic unit of attention is the scaled dot-product attention, which computes the attention scores between queries (Q), keys (K), and values (V). The scores determine how much focus to place on different parts of the input sequence.\n",
        "   - The attention mechanism is defined as:\n",
        "     $\n",
        "     \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "     $\n",
        "     where $d_k$ is the dimension of the keys.\n",
        "\n",
        "2. **Multi-Head Mechanism:**\n",
        "   - Instead of performing a single attention function, the multi-head mechanism splits the queries, keys, and values into multiple heads, each of which performs attention separately. The results are then concatenated and linearly transformed to produce the final output.\n",
        "   - This allows the model to capture different types of dependencies and relationships in the input sequence.\n",
        "\n",
        "3. **Implementation:**\n",
        "   - The multi-head attention is implemented by first linearly projecting the queries, keys, and values $h$ times with different learned projections, then applying the scaled dot-product attention in parallel, and finally concatenating the results.\n",
        "\n",
        "The multi-head attention mechanism enhances the model's ability to focus on different parts of the sequence and learn various aspects of the data simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "et0Bibf6Capr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_embedding, n_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_embedding % n_heads == 0\n",
        "\n",
        "        self.d_embedding = d_embedding\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_embedding // n_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_embedding, d_embedding)\n",
        "        self.W_k = nn.Linear(d_embedding, d_embedding)\n",
        "        self.W_v = nn.Linear(d_embedding, d_embedding)\n",
        "        self.W_o = nn.Linear(d_embedding, d_embedding)\n",
        "\n",
        "    def compute_attention_probs(self, Q, K, V, mask=None):\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "\n",
        "    def make_heads(self, x):\n",
        "        batch_size, seq_length, d_embedding = x.size()\n",
        "        y = x.view(batch_size, seq_length, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        return x.view(batch_size, seq_length, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def join_heads(self, x):\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_embedding)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        Q = self.make_heads(self.W_q(Q)) # Replace Q with Q*W^q\n",
        "        K = self.make_heads(self.W_k(K)) # Replace K with K*W^k\n",
        "        V = self.make_heads(self.W_v(V)) # Replace V with V*W^v\n",
        "\n",
        "        x = self.compute_attention_probs(Q, K, V, mask) # Compute attention scores\n",
        "        x = self.join_heads(x) # Join the heads (concatenation)\n",
        "        x = self.W_o(x) # Final linear layer\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2GmdYLaCapr"
      },
      "source": [
        "### Feedforward Neural Network\n",
        "\n",
        "The feedforward neural network (FFN) is another crucial component in both the encoder and decoder blocks of the Transformer model. It operates on each position of the sequence independently and identically. Here's an overview of the feedforward network:\n",
        "\n",
        "1. **Two Linear Layers with ReLU Activation:**\n",
        "   - The FFN consists of two linear transformations with a ReLU activation in between. This introduces non-linearity into the model, enabling it to capture more complex patterns in the data.\n",
        "   - The transformation is defined as:\n",
        "     $\n",
        "      \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
        "     $\n",
        "     where $W_1$ and $W_2$ are weight matrices, and $b_1$ and $b_2$ are bias terms.\n",
        "\n",
        "2. **Position-wise Application:**\n",
        "   - Unlike traditional feedforward layers that are applied to the entire input, the FFN in the Transformer is applied to each position separately and identically. This means that the same feedforward network is used for each token in the sequence.\n",
        "\n",
        "3. **Dimension Expansion and Reduction:**\n",
        "   - The first linear layer expands the dimension from $d_{\\text{model}}$ to a higher dimension (typically four times larger), and the second linear layer reduces it back to $d_{\\text{model}}$. This allows the model to learn more complex representations within the higher-dimensional space.\n",
        "\n",
        "The feedforward neural network helps the Transformer model to process and transform the input representations, adding another layer of abstraction and enabling it to learn more sophisticated features from the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "B4oHgXX-Capr"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_embedding, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_embedding, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_embedding)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2l97gJjCapr"
      },
      "source": [
        "## Encoder block\n",
        "\n",
        "The first part of the Transformer mechanism comprises the Encoder block. This block, as the name suggests, is in charge of producing intermediate encodings for the input which the model decodes in the next part, which is the Decoder block.\n",
        "\n",
        "Each encoder block consists of the following components:\n",
        "\n",
        "1. **Multi-head Self-Attention Mechanism:** This mechanism allows the model to focus on different parts of the input sequence when encoding a particular token. By using multiple heads, the model can capture various aspects of the relationships between tokens.\n",
        "\n",
        "2. **Feedforward Neural Network:** A fully connected feedforward neural network is applied to each position separately and identically. It consists of two linear transformations with a ReLU activation in between.\n",
        "\n",
        "3. **Layer Normalization:** Layer normalization is applied after the self-attention and feedforward networks to stabilize and speed up the training process.\n",
        "\n",
        "4. **Residual Connections (Skip Connections):** Residual connections are used to help the flow of gradients during backpropagation. The input to each sub-layer is added to its output, and the result is normalized.\n",
        "\n",
        "The combination of these components allows the encoder to effectively capture the dependencies and relationships within the input sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "W3_IuzJpCaps"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, d_embedding, n_heads, d_ff, dropout):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_embedding, n_heads)\n",
        "        self.feed_forward = FeedForward(d_embedding, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_embedding)\n",
        "        self.norm2 = nn.LayerNorm(d_embedding)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        Returns: The output of encoder on a given input batch x\n",
        "        \"\"\"\n",
        "        # First, pass the input through the multi-head self-attention layer\n",
        "        attn_output = self.self_attention(x, x, x, mask)\n",
        "\n",
        "        # Apply dropout to the attention output\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "\n",
        "        # Add the input x to the output of the dropout layer as a residual connection, then apply layer normalization\n",
        "        x = self.norm1(x + attn_output)\n",
        "\n",
        "        # Pass the output through the feed-forward network\n",
        "        ff_output = self.feed_forward(x)\n",
        "\n",
        "        # Apply dropout to the feed-forward network output\n",
        "        ff_output = self.dropout2(ff_output)\n",
        "\n",
        "        # Add the output of the feed-forward layer to x as a residual connection, then apply layer normalization\n",
        "        x = self.norm2(x + ff_output)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo6W7bqSCaps"
      },
      "source": [
        "### Decoder Block\n",
        "\n",
        "The decoder block in the Transformer model is designed to generate output sequences by attending to both the encoder outputs and the previously generated tokens. Each decoder block consists of the following components:\n",
        "\n",
        "1. **Masked Multi-head Self-Attention Mechanism:** This mechanism allows the decoder to attend to different parts of the output sequence generated so far while ensuring that predictions for a particular position depend only on the known outputs at positions before it.\n",
        "\n",
        "2. **Multi-head Attention Mechanism:** This layer attends to the encoder's output, allowing the decoder to focus on relevant parts of the input sequence. It helps the decoder to use information from the entire input sequence while generating each token of the output.\n",
        "\n",
        "3. **Feedforward Neural Network:** Similar to the encoder, a fully connected feedforward neural network is applied to each position separately and identically.\n",
        "\n",
        "4. **Layer Normalization:** Layer normalization is applied after each attention and feedforward network to stabilize and speed up the training process.\n",
        "\n",
        "5. **Residual Connections (Skip Connections):** Residual connections are used to facilitate the flow of gradients during backpropagation. The input to each sub-layer is added to its output, and the result is normalized.\n",
        "\n",
        "These components enable the decoder to generate coherent and contextually appropriate sequences by attending to the encoder outputs and the previously generated tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "tRlFrWYgCaps"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, d_embedding, n_heads, d_ff, dropout):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        \"\"\"\n",
        "        Args:\n",
        "           self_attention: decoder's multi-head attention\n",
        "           cross_attention: The multi-head attention layer between the encoder and the decoder\n",
        "           feed_forward: feed-forward layer\n",
        "           mask: mask to be given for multi head attention\n",
        "           norm1: First Normalization layer\n",
        "           norm2: Second Normalization layer\n",
        "           norm3: Third Normalization layer\n",
        "           dropout: Final dropout layer\n",
        "        \"\"\"\n",
        "        self.self_attention = MultiHeadAttention(d_embedding, n_heads)\n",
        "        self.cross_attention = MultiHeadAttention(d_embedding, n_heads)\n",
        "        self.feed_forward = FeedForward(d_embedding, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_embedding)\n",
        "        self.norm2 = nn.LayerNorm(d_embedding)\n",
        "        self.norm3 = nn.LayerNorm(d_embedding)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        \"\"\"\n",
        "        Returns: The output of decoder on a given input batch x\n",
        "        \"\"\"\n",
        "        # Self-attention\n",
        "        attn_output = self.self_attention(x, x, x, tgt_mask)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        x = self.norm1(x + attn_output)  # Apply residual connection and normalize\n",
        "\n",
        "        # Cross-attention\n",
        "        cross_attn_output = self.cross_attention(x, enc_output, enc_output, src_mask)\n",
        "        cross_attn_output = self.dropout2(cross_attn_output)\n",
        "        x = self.norm2(x + cross_attn_output)  # Apply residual connection and normalize\n",
        "\n",
        "        # Feed-forward\n",
        "        ff_output = self.feed_forward(x)\n",
        "        ff_output = self.dropout3(ff_output)\n",
        "        x = self.norm3(x + ff_output)  # Apply residual connection and normalize\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95CXQyOqCaps"
      },
      "source": [
        "## Positional encoding\n",
        "\n",
        "The last part we need to implement the Transformer block is the positional encoding block. Positional encoding is an essential part of the transformer because it helps assign semantic weightage to words depending on their order (or \"position\") in the sequence - therefore, words \"closer\" to the current word will have more \"importance\" and vice versa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "tAHqVk8ZCaps"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_embedding, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_embedding)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_embedding, 2).float() * -(math.log(10000.0) / d_embedding))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDR4Z9q_Capt"
      },
      "source": [
        "## Putting It All Together\n",
        "\n",
        "The Transformer model consists of a stack of identical encoder and decoder blocks. The encoder processes the input sequence and generates a set of attention-based representations. The decoder then uses these representations, along with the previously generated tokens, to produce the output sequence.\n",
        "\n",
        "By leveraging self-attention and multi-head attention mechanisms, the Transformer model can capture complex dependencies and relationships within the data, making it highly effective for various sequence modeling tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "pIm9eCztCapt"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_embedding, n_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_embedding)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_embedding)\n",
        "        self.positional_encoding = PositionalEncoding(d_embedding, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderBlock(d_embedding, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderBlock(d_embedding, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(d_embedding, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # Masking the input; the outputs returned by this function should be passed to encoder and decoder layers\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        \"\"\"\n",
        "        Returns: The output of transformer on a given input batch x\n",
        "        \"\"\"\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "\n",
        "        # Pass the input through the encoder layers\n",
        "        x = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            x = encoder_layer(x, src_mask)\n",
        "\n",
        "        # Pass the input through the decoder layers\n",
        "        y = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "        for decoder_layer in self.decoder_layers:\n",
        "            y = decoder_layer(y, x, src_mask, tgt_mask)\n",
        "\n",
        "        # Apply dropout to the output of the decoder layers\n",
        "        # y = self.dropout(y)\n",
        "        y = self.fc(y)\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zibSdNGwCapt"
      },
      "source": [
        "## Training\n",
        "\n",
        "We train the Transformer model using appropriate loss functions and optimization techniques. The model aims to learn patterns in the sequence data and improve its performance on the given task over multiple epochs. We will track the training process and monitor the model's performance. Since the loss function output goes down with the epochs, we understand that the transformer is training correctly and behaving as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBd0IJLiCapt",
        "outputId": "a70b0dfb-e1a6-4d67-dcbd-e573e83873be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/25], Loss: 8.6774\n",
            "Epoch [2/25], Loss: 8.5359\n",
            "Epoch [3/25], Loss: 8.4435\n",
            "Epoch [4/25], Loss: 8.3526\n",
            "Epoch [5/25], Loss: 8.2394\n",
            "Epoch [6/25], Loss: 8.1204\n",
            "Epoch [7/25], Loss: 7.9994\n",
            "Epoch [8/25], Loss: 7.8808\n",
            "Epoch [9/25], Loss: 7.7657\n",
            "Epoch [10/25], Loss: 7.6479\n",
            "Epoch [11/25], Loss: 7.5275\n",
            "Epoch [12/25], Loss: 7.4031\n",
            "Epoch [13/25], Loss: 7.2949\n",
            "Epoch [14/25], Loss: 7.1699\n",
            "Epoch [15/25], Loss: 7.0571\n",
            "Epoch [16/25], Loss: 6.9397\n",
            "Epoch [17/25], Loss: 6.8277\n",
            "Epoch [18/25], Loss: 6.7123\n",
            "Epoch [19/25], Loss: 6.5990\n",
            "Epoch [20/25], Loss: 6.4911\n",
            "Epoch [21/25], Loss: 6.3798\n",
            "Epoch [22/25], Loss: 6.2721\n",
            "Epoch [23/25], Loss: 6.1610\n",
            "Epoch [24/25], Loss: 6.0599\n",
            "Epoch [25/25], Loss: 5.9536\n"
          ]
        }
      ],
      "source": [
        "src_vocab_size = 5000\n",
        "tgt_vocab_size = 5000\n",
        "d_embedding = 512\n",
        "n_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "max_seq_length = 100\n",
        "dropout = 0.1\n",
        "EPOCHS = 25\n",
        "training_loss = []\n",
        "\n",
        "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_embedding, n_heads, num_layers, d_ff, max_seq_length, dropout)\n",
        "\n",
        "# Generate random sample data\n",
        "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))\n",
        "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "transformer.train()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    optimizer.zero_grad()\n",
        "    output = transformer(src_data, tgt_data)\n",
        "    loss = criterion(output.view(-1, tgt_vocab_size), tgt_data.view(-1))\n",
        "    # output = transformer(src_data, tgt_data[:, :-1])\n",
        "    # loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    training_loss.append(loss.item())\n",
        "\n",
        "torch.save(transformer.state_dict(), \"./etc/transformer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "SSC7uRb3Capt",
        "outputId": "3725d418-0fe6-4b01-ef23-412cc04eb3e8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(list(range(1, EPOCHS+1)), training_loss)\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}